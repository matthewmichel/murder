# murder — Product Requirements Document

> **murder** — a group or flock of agents
>
> Humans steer. Agents execute.

Murder is a local CLI tool that gives developers a way to set up agent-first development on their existing projects. The product isn't the agents — it's the environment that makes agents effective. Run `murder init` on your existing project, and your codebase becomes agent-ready. Run `murder task`, and agents do the work.

---

A Blog Post by the OpenAI Team that inspired this workflow:

Harness engineering: leveraging Codex in an agent-first world
By Ryan Lopopolo, Member of the Technical Staff

Over the past five months, our team has been running an experiment: building and shipping an internal beta of a software product with 0 lines of manually-written code.

The product has internal daily users and external alpha testers. It ships, deploys, breaks, and gets fixed. What’s different is that every line of code—application logic, tests, CI configuration, documentation, observability, and internal tooling—has been written by Codex. We estimate that we built this in about 1/10th the time it would have taken to write the code by hand.

Humans steer. Agents execute.

We intentionally chose this constraint so we would build what was necessary to increase engineering velocity by orders of magnitude. We had weeks to ship what ended up being a million lines of code. To do that, we needed to understand what changes when a software engineering team’s primary job is no longer to write code, but to design environments, specify intent, and build feedback loops that allow Codex agents to do reliable work.

This post is about what we learned by building a brand new product with a team of agents—what broke, what compounded, and how to maximize our one truly scarce resource: human time and attention.

We started with an empty git repository
The first commit to an empty repository landed in late August 2025.

The initial scaffold—repository structure, CI configuration, formatting rules, package manager setup, and application framework—was generated by Codex CLI using GPT‑5, guided by a small set of existing templates. Even the initial AGENTS.md file that directs agents how to work in the repository was itself written by Codex.

There was no pre-existing human-written code to anchor the system. From the beginning, the repository was shaped by the agent.

Five months later, the repository contains on the order of a million lines of code across application logic, infrastructure, tooling, documentation, and internal developer utilities. Over that period, roughly 1,500 pull requests have been opened and merged with a small team of just three engineers driving Codex. This translates to an average throughput of 3.5 PRs per engineer per day, and surprisingly the throughput has increased as the team has grown to now seven engineers. Importantly, this wasn’t output for output’s sake: the product has been used by hundreds of users internally, including daily internal power users.

Throughout the development process, humans never directly contributed any code. This became a core philosophy for the team: no manually-written code.

Redefining the role of the engineer
The lack of hands-on human coding introduced a different kind of engineering work, focused on systems, scaffolding, and leverage.

Early progress was slower than we expected, not because Codex was incapable, but because the environment was underspecified. The agent lacked the tools, abstractions, and internal structure required to make progress toward high-level goals. The primary job of our engineering team became enabling the agents to do useful work.

In practice, this meant working depth-first: breaking down larger goals into smaller building blocks (design, code, review, test, etc), prompting the agent to construct those blocks, and using them to unlock more complex tasks. When something failed, the fix was almost never “try harder.” Because the only way to make progress was to get Codex to do the work, human engineers always stepped into the task and asked: “what capability is missing, and how do we make it both legible and enforceable for the agent?”

Humans interact with the system almost entirely through prompts: an engineer describes a task, runs the agent, and allows it to open a pull request. To drive a PR to completion, we instruct Codex to review its own changes locally, request additional specific agent reviews both locally and in the cloud, respond to any human or agent given feedback, and iterate in a loop until all agent reviewers are satisfied (effectively this is a Ralph Wiggum Loop⁠(opens in a new window)). Codex uses our standard development tools directly (gh, local scripts, and repository-embedded skills) to gather context without humans copying and pasting into the CLI.

Humans may review pull requests, but aren’t required to. Over time, we’ve pushed almost all review effort towards being handled agent-to-agent.

Increasing application legibility
As code throughput increased, our bottleneck became human QA capacity. Because the fixed constraint has been human time and attention, we’ve worked to add more capabilities to the agent by making things like the application UI, logs, and app metrics themselves directly legible to Codex.

For example, we made the app bootable per git worktree, so Codex could launch and drive one instance per change. We also wired the Chrome DevTools Protocol into the agent runtime and created skills for working with DOM snapshots, screenshots, and navigation. This enabled Codex to reproduce bugs, validate fixes, and reason about UI behavior directly.

Diagram titled “Codex drives the app with Chrome DevTools MCP to validate its work.” Codex selects a target, snapshots the state before and after triggering a UI path, observes runtime events via Chrome DevTools, applies fixes, restarts, and loops re-running validation until the app is clean.
We did the same for observability tooling. Logs, metrics, and traces are exposed to Codex via a local observability stack that’s ephemeral for any given worktree. Codex works on a fully isolated version of that app—including its logs and metrics, which get torn down once that task is complete. Agents can query logs with LogQL and metrics with PromQL. With this context available, prompts like “ensure service startup completes in under 800ms” or “no span in these four critical user journeys exceeds two seconds” become tractable.

Diagram titled “Giving Codex a full observability stack in local dev.” An app sends logs, metrics, and traces to Vector, which fans out data to an observability stack containing Victoria Logs, Metrics, and Traces, each queried via LogQL, PromQL, or TraceQL APIs. Codex uses these signals to query, correlate, and reason, then implements fixes in the codebase, restarts the app, re-runs workloads, tests UI journeys, and repeats in a feedback loop.
We regularly see single Codex runs work on a single task for upwards of six hours (often while the humans are sleeping).

We made repository knowledge the system of record
Context management is one of the biggest challenges in making agents effective at large and complex tasks. One of the earliest lessons we learned was simple: give Codex a map, not a 1,000-page instruction manual.

We tried the “one big AGENTS.md⁠(opens in a new window)” approach. It failed in predictable ways:

Context is a scarce resource. A giant instruction file crowds out the task, the code, and the relevant docs—so the agent either misses key constraints or starts optimizing for the wrong ones.
Too much guidance becomes non-guidance. When everything is “important,” nothing is. Agents end up pattern-matching locally instead of navigating intentionally.
It rots instantly. A monolithic manual turns into a graveyard of stale rules. Agents can’t tell what’s still true, humans stop maintaining it, and the file quietly becomes an attractive nuisance.
It’s hard to verify. A single blob doesn’t lend itself to mechanical checks (coverage, freshness, ownership, cross-links), so drift is inevitable.
So instead of treating AGENTS.md as the encyclopedia, we treat it as the table of contents.

The repository’s knowledge base lives in a structured docs/ directory treated as the system of record. A short AGENTS.md (roughly 100 lines) is injected into context and serves primarily as a map, with pointers to deeper sources of truth elsewhere.

Plain Text

1
AGENTS.md
2
ARCHITECTURE.md
3
docs/
4
├── design-docs/
5
│   ├── index.md
6
│   ├── core-beliefs.md
7
│   └── ...
8
├── exec-plans/
9
│   ├── active/
10
│   ├── completed/
11
│   └── tech-debt-tracker.md
12
├── generated/
13
│   └── db-schema.md
14
├── product-specs/
15
│   ├── index.md
16
│   ├── new-user-onboarding.md
17
│   └── ...
18
├── references/
19
│   ├── design-system-reference-llms.txt
20
│   ├── nixpacks-llms.txt
21
│   ├── uv-llms.txt
22
│   └── ...
23
├── DESIGN.md
24
├── FRONTEND.md
25
├── PLANS.md
26
├── PRODUCT_SENSE.md
27
├── QUALITY_SCORE.md
28
├── RELIABILITY.md
29
└── SECURITY.md
In-repository knowledge store layout.

Design documentation is catalogued and indexed, including verification status and a set of core beliefs that define agent-first operating principles. Architecture documentation⁠(opens in a new window) provides a top-level map of domains and package layering. A quality document grades each product domain and architectural layer, tracking gaps over time.

Plans are treated as first-class artifacts. Ephemeral lightweight plans are used for small changes, while complex work is captured in execution plans⁠(opens in a new window) with progress and decision logs that are checked into the repository. Active plans, completed plans, and known technical debt are all versioned and co-located, allowing agents to operate without relying on external context.

This enables progressive disclosure: agents start with a small, stable entry point and are taught where to look next, rather than being overwhelmed up front.

We enforce this mechanically. Dedicated linters and CI jobs validate that the knowledge base is up to date, cross-linked, and structured correctly. A recurring “doc-gardening” agent scans for stale or obsolete documentation that does not reflect the real code behavior and opens fix-up pull requests.

Agent legibility is the goal
As the codebase evolved, Codex’s framework for design decisions needed to evolve, too.

Because the repository is entirely agent-generated, it’s optimized first for Codex’s legibility. In the same way teams aim to improve navigability of their code for new engineering hires, our human engineers’ goal was making it possible for an agent to reason about the full business domain directly from the repository itself.

From the agent’s point of view, anything it can’t access in-context while running effectively doesn’t exist. Knowledge that lives in Google Docs, chat threads, or people’s heads are not accessible to the system. Repository-local, versioned artifacts (e.g., code, markdown, schemas, executable plans) are all it can see.

Diagram titled “The limits of agent knowledge: What Codex can’t see doesn’t exist.” Codex’s knowledge is shown as a bounded bubble. Below it are examples of unseen knowledge—Google Docs, Slack messages, and tacit human knowledge. Arrows indicate that to make this information visible to Codex, it must be encoded into the codebase as markdown.
We learned that we needed to push more and more context into the repo over time. That Slack discussion that aligned the team on an architectural pattern? If it isn’t discoverable to the agent, it’s illegible in the same way it would be unknown to a new hire joining three months later.

Giving Codex more context means organizing and exposing the right information so the agent can reason over it, rather than overwhelming it with ad-hoc instructions. In the same way you would onboard a new teammate on product principles, engineering norms, and team culture (emoji preferences included), giving the agent this information leads to better-aligned output.

This framing clarified many tradeoffs. We favored dependencies and abstractions that could be fully internalized and reasoned about in-repo. Technologies often described as “boring” tend to be easier for agents to model due to composability, api stability, and representation in the training set. In some cases, it was cheaper to have the agent reimplement subsets of functionality than to work around opaque upstream behavior from public libraries. For example, rather than pulling in a generic p-limit-style package, we implemented our own map-with-concurrency helper: it’s tightly integrated with our OpenTelemetry instrumentation, has 100% test coverage, and behaves exactly the way our runtime expects.

Pulling more of the system into a form the agent can inspect, validate, and modify directly increases leverage—not just for Codex, but for other agents (e.g. Aardvark) that are working on the codebase as well.

Enforcing architecture and taste
Documentation alone doesn’t keep a fully agent-generated codebase coherent. By enforcing invariants, not micromanaging implementations, we let agents ship fast without undermining the foundation. For example, we require Codex to parse data shapes at the boundary⁠(opens in a new window), but are not prescriptive on how that happens (the model seems to like Zod, but we didn’t specify that specific library).

Agents are most effective in environments with strict boundaries and predictable structure⁠(opens in a new window), so we built the application around a rigid architectural model. Each business domain is divided into a fixed set of layers, with strictly validated dependency directions and a limited set of permissible edges. These constraints are enforced mechanically via custom linters (Codex-generated, of course!) and structural tests.

The diagram below shows the rule: within each business domain (e.g. App Settings), code can only depend “forward” through a fixed set of layers (Types → Config → Repo → Service → Runtime → UI). Cross-cutting concerns (auth, connectors, telemetry, feature flags) enter through a single explicit interface: Providers. Anything else is disallowed and enforced mechanically.

Diagram titled “Layered domain architecture with explicit cross-cutting boundaries.” Inside the business logic domain are modules: Types → Config → Repo, and Providers → Service → Runtime → UI, with App Wiring + UI at the bottom. A Utils module sits outside the boundary and feeds into Providers.
This is the kind of architecture you usually postpone until you have hundreds of engineers. With coding agents, it’s an early prerequisite: the constraints are what allows speed without decay or architectural drift.

In practice, we enforce these rules with custom linters and structural tests, plus a small set of “taste invariants.” For example, we statically enforce structured logging, naming conventions for schemas and types, file size limits, and platform-specific reliability requirements with custom lints. Because the lints are custom, we write the error messages to inject remediation instructions into agent context.

In a human-first workflow, these rules might feel pedantic or constraining. With agents, they become multipliers: once encoded, they apply everywhere at once.

At the same time, we’re explicit about where constraints matter and where they do not. This resembles leading a large engineering platform organization: enforce boundaries centrally, allow autonomy locally. You care deeply about boundaries, correctness, and reproducibility. Within those boundaries, you allow teams—or agents—significant freedom in how solutions are expressed.

The resulting code does not always match human stylistic preferences, and that’s okay. As long as the output is correct, maintainable, and legible to future agent runs, it meets the bar.

Human taste is fed back into the system continuously. Review comments, refactoring pull requests, and user-facing bugs are captured as documentation updates or encoded directly into tooling. When documentation falls short, we promote the rule into code

Throughput changes the merge philosophy
As Codex’s throughput increased, many conventional engineering norms became counterproductive.

The repository operates with minimal blocking merge gates. Pull requests are short-lived. Test flakes are often addressed with follow-up runs rather than blocking progress indefinitely. In a system where agent throughput far exceeds human attention, corrections are cheap, and waiting is expensive.

This would be irresponsible in a low-throughput environment. Here, it’s often the right tradeoff.

What “agent-generated” actually means
When we say the codebase is generated by Codex agents, we mean everything in the codebase.

Agents produce:

Product code and tests
CI configuration and release tooling
Internal developer tools
Documentation and design history
Evaluation harnesses
Review comments and responses
Scripts that manage the repository itself
Production dashboard definition files
Humans always remain in the loop, but work at a different layer of abstraction than we used to. We prioritize work, translate user feedback into acceptance criteria, and validate outcomes. When the agent struggles, we treat it as a signal: identify what is missing—tools, guardrails, documentation—and feed it back into the repository, always by having Codex itself write the fix.

Agents use our standard development tools directly. They pull review feedback, respond inline, push updates, and often squash and merge their own pull requests.

Increasing levels of autonomy
As more of the development loop was encoded directly into the system—testing, validation, review, feedback handling, and recovery—the repository recently crossed a meaningful threshold where Codex can end-to-end drive a new feature.

Given a single prompt, the agent can now:

Validate the current state of the codebase
Reproduce a reported bug
Record a video demonstrating the failure
Implement a fix
Validate the fix by driving the application
Record a second video demonstrating the resolution
Open a pull request
Respond to agent and human feedback
Detect and remediate build failures
Escalate to a human only when judgment is required
Merge the change
This behavior depends heavily on the specific structure and tooling of this repository and should not be assumed to generalize without similar investment—at least, not yet.

Entropy and garbage collection
Full agent autonomy also introduces novel problems. Codex replicates patterns that already exist in the repository—even uneven or suboptimal ones. Over time, this inevitably leads to drift.

Initially, humans addressed this manually. Our team used to spend every Friday (20% of the week) cleaning up “AI slop.” Unsurprisingly, that didn’t scale.

Instead, we started encoding what we call “golden principles” directly into the repository and built a recurring cleanup process. These principles are opinionated, mechanical rules that keep the codebase legible and consistent for future agent runs. For example: (1) we prefer shared utility packages over hand-rolled helpers to keep invariants centralized, and (2) we don’t probe data “YOLO-style”—we validate boundaries or rely on typed SDKs so the agent can’t accidentally build on guessed shapes. On a regular cadence, we have a set of background Codex tasks that scan for deviations, update quality grades, and open targeted refactoring pull requests. Most of these can be reviewed in under a minute and automerged.

This functions like garbage collection. Technical debt is like a high-interest loan: it’s almost always better to pay it down continuously in small increments than to let it compound and tackle it in painful bursts. Human taste is captured once, then enforced continuously on every line of code. This also lets us catch and resolve bad patterns on a daily basis, rather than letting them spread in the code base for days or weeks.

What we’re still learning
This strategy has so far worked well up through internal launch and adoption at OpenAI. Building a real product for real users helped anchor our investments in reality and guide us towards long-term maintainability.

What we don’t yet know is how architectural coherence evolves over years in a fully agent-generated system. We’re still learning where human judgment adds the most leverage and how to encode that judgment so it compounds. We also don’t know how this system will evolve as models continue to become more capable over time.

What’s become clear: building software still demands discipline, but the discipline shows up more in the scaffolding rather than the code. The tooling, abstractions, and feedback loops that keep the codebase coherent are increasingly important.

Our most difficult challenges now center on designing environments, feedback loops, and control systems that help agents accomplish our goal: build and maintain complex, reliable software at scale.

As agents like Codex take on larger portions of the software lifecycle, these questions will matter even more. We hope that sharing some early lessons helps you reason about where to invest your effort so you can just build things.

---

## Core Insight

Agents fail not because they're incapable, but because the environment is underspecified. The primary job of murder is to make codebases legible to agents, orchestrate their work, and build feedback loops so they improve over time.

Three principles guide everything:

1. **Give agents a map, not a manual** — short entry points that link to deeper docs, not monolithic instruction files
2. **Enforce boundaries, allow autonomy** — strict architectural constraints with freedom in implementation
3. **Repository as system of record** — if it's not in the repo, it doesn't exist to the agent

---

## Current State (v0.1)

What exists today:

- [x] CLI framework (`murder start`, `stop`, `setup`, `project`, `reset`, `help`)
- [x] Docker-based Postgres 18 with pgvector and pg_cron
- [x] Database schema (projects, providers, keys, configs, conversations, memories, agent backends)
- [x] Web UI for management (dashboard, providers, configs, agents, projects)
- [x] Agent detection and registration (Claude Code, Cursor CLI, OpenAI Codex)
- [x] AI provider configuration (encrypted API keys, model selection)
- [x] Memory system integration (mem0 + pgvector)
- [x] Project registration via `murder project`

What's missing: the part that actually shapes a project for agent-driven development and dispatches work.

---

## Feature Roadmap

### Phase 1: Repository Scaffolding — `murder init`

**Priority: Highest — everything else depends on this.**

When a user runs `murder init` from their project directory, murder should scan the codebase and generate an agent-legible knowledge structure. This is the single highest-leverage feature because agents need context to do useful work.

#### Requirements

- [x] **Codebase analysis** — Detect language(s), framework, package manager, directory structure, entry points, test runner, CI config, linter config
- [x] **Generate `AGENTS.md`** — A short (~100 line) table of contents that points agents to deeper docs. Not a monolith. A map with pointers to the right context for the right task. *(Generated at project root)*
- [x] **Generate `.murder/ARCHITECTURE.md`** — Auto-detected module boundaries, key directories and their purposes, dependency graph, entry points, data flow
- [x] **Create `.murder/` knowledge structure** — Structured knowledge base inside the user's project:
  ```
  .murder/
  ├── design-docs/
  ├── exec-plans/
  │   ├── active/
  │   └── completed/
  ├── references/
  ├── logs/
  ├── ARCHITECTURE.md
  ├── config.ts
  └── core-beliefs.md
  ```
- [x] **Seed core beliefs** — Pull conventions from existing config files (tsconfig, eslint, prettier, .editorconfig, etc.) and encode them as agent-readable principles
- [x] **Generate `.murder/config.ts`** — Project-level config file that defines validation commands, boot sequence, and project metadata
- [x] ~~**Store project metadata in murder DB**~~ — Not needed. The `.murder/` directory is the system of record (config.ts, ARCHITECTURE.md, core-beliefs.md). Duplicating into DB would create drift.
- [x] **Idempotent** — Running `murder init` again prompts before overwriting; scaffolding directories are created if missing
- [x] **Register as project** — If the directory isn't already a murder project, register it automatically

#### Technical Notes

- Use the orchestration model (configured via `murder setup`) to analyze the codebase and generate docs
- Feed directory listings, package.json/Cargo.toml/pyproject.toml, config files, and a sampling of source files as context
- The generated files live in the USER's repo (not in murder's database) — the repo is the system of record
- murder.config.ts is the bridge between the user's project and murder's orchestration capabilities

---

### Phase 2: Task Dispatch — `murder task`

**Priority: High — the core value proposition.**

Accept a natural language task, provide agent context, and dispatch to an available agent backend.

#### Requirements

- [ ] **`murder task "<prompt>"`** — Accept a task description from the CLI
- [ ] **Context assembly** — Before dispatching, gather: AGENTS.md, relevant ARCHITECTURE.md sections, relevant memories from pgvector, murder.config.ts validation commands
- [ ] **Single-agent dispatch** — Send the task + context to the default agent backend (claude, codex, or cursor agent CLI)
- [ ] **Conversation tracking** — Record the task, agent used, start/end time, status in the conversations table
- [ ] **Validation loop** — After agent completes, automatically run validation commands from murder.config.ts. If failures, feed errors back to the agent and loop.
- [ ] **`murder tasks`** — List active and recent tasks
- [ ] **`murder status`** — Show what's currently running

#### Technical Notes

- Start with single-agent dispatch. Multi-agent comes in Phase 4.
- Each agent backend has a different CLI interface. Abstract behind a common dispatch interface.
- Validation loop has a max retry count (default 3) to prevent infinite loops.

---

### Phase 3: Memory & Learning

**Priority: High — makes past work compound.**

The memory system (mem0 + pgvector) exists but nothing writes to it or reads from it during task execution.

#### Requirements

- [ ] **Post-task memory extraction** — After every completed task, use the orchestration model to extract: what was done, what patterns were used, what broke and how it was fixed, any architectural decisions made
- [ ] **Memory injection on task start** — Before dispatching a task, query pgvector for relevant memories and include them in agent context
- [ ] **Convention learning** — As agents work, detect and store project conventions ("uses Zod for validation", "API routes in src/api/", "vitest not jest")
- [ ] **Error pattern storage** — Store validation failures and their resolutions so agents don't repeat mistakes
- [ ] **Memory management in web UI** — View, search, and delete memories per project

---

### Phase 4: Multi-Agent Orchestration

**Priority: Medium — speed multiplier.**

Decompose large tasks and run multiple agents in parallel.

#### Requirements

- [ ] **Task decomposition** — Use orchestration model to break a task into independent subtasks
- [ ] **Parallel dispatch** — Run subtasks on different agent backends simultaneously
- [ ] **Dependency tracking** — Some subtasks depend on others. Execute in correct order.
- [ ] **Agent-to-agent review** — After one agent completes work, dispatch a second agent to review it
- [ ] **Conflict resolution** — When parallel agents modify the same files, detect and resolve conflicts
- [ ] **`murder queue`** — Enqueue tasks for background execution

#### Technical Notes

- pg_cron can drive the background queue
- Git worktrees enable parallel agents working on isolated branches
- Each agent gets its own worktree, merges happen through PRs

---

### Phase 5: Feedback Loops & Self-Validation

**Priority: Medium — increases autonomy.**

Give agents the ability to boot the app, drive the UI, check logs, and validate their own work beyond just running test commands.

#### Requirements

- [ ] **App boot per worktree** — Each agent can start an isolated instance of the app
- [ ] **Health check verification** — Confirm the app boots and responds on the configured port
- [ ] **Screenshot/DOM capture** — For web apps, capture screenshots before and after changes
- [ ] **Log inspection** — Expose application logs to agents so they can debug runtime issues
- [ ] **Performance gates** — "Build must complete in under 30s", "No test suite regression"

---

### Phase 6: Garbage Collection — `murder sweep`

**Priority: Lower — long-term maintainability.**

Automated quality maintenance to prevent entropy in agent-generated code.

#### Requirements

- [ ] **`murder sweep`** — Run a quality scan across the project using the orchestration model
- [ ] **Stale doc detection** — Find docs that don't match current code behavior
- [ ] **Pattern consistency** — Detect inconsistent patterns and suggest consolidation
- [ ] **Quality scoring** — Track quality metrics per project over time (visible in web UI)
- [ ] **Tech debt tracking** — Auto-update .murder/docs/tech-debt.md with known issues
- [ ] **Scheduled sweeps** — Use pg_cron to run sweeps on a configurable cadence

---

### Phase 7: Knowledge Management

**Priority: Lower — compounds over time.**

Keep the agent knowledge base fresh and accurate.

#### Requirements

- [ ] **Doc gardening agent** — Periodic scans for stale or obsolete documentation
- [ ] **Auto-update ARCHITECTURE.md** — When significant structural changes land, regenerate
- [ ] **Design doc templates** — `murder design "<feature>"` creates a structured design doc in .murder/docs/design-docs/
- [ ] **Exec plan lifecycle** — Move plans from active/ to completed/ as tasks finish
- [ ] **Cross-linking validation** — Ensure docs reference each other correctly

---

## Build Order

| # | Feature | Depends On | Est. Complexity | Status |
|---|---------|------------|-----------------|--------|
| 1 | `murder init` — codebase analysis | Existing infra | Medium | Done |
| 2 | `murder init` — AGENTS.md generation | #1 | Medium | Done |
| 3 | `murder init` — ARCHITECTURE.md generation | #1 | Medium | Done |
| 4 | `murder init` — knowledge structure scaffolding | #1 | Low | Done |
| 5 | `murder init` — config.ts generation | #1 | Low | Done |
| 6 | `murder task` — single-agent dispatch | #5 | High | |
| 7 | `murder task` — validation loop | #5, #6 | Medium | |
| 8 | Memory injection on task start | #6 | Medium | |
| 9 | Post-task memory extraction | #6 | Medium | |
| 10 | `murder tasks` / `murder status` | #6 | Low | |
| 11 | Agent-to-agent review | #6 | High | |
| 12 | Multi-agent parallel dispatch | #6 | High | |
| 13 | `murder sweep` | #6, #8 | Medium | |
| 14 | Web UI task dashboard | #6, #10 | Medium | |
| 15 | Doc gardening | #4 | Low | |

---

## Immediate Next Step

Build `murder task`. Phase 1 (`murder init`) is complete — projects can be scanned, scaffolded, and registered. The next highest-value feature is single-agent task dispatch: accept a prompt, assemble context from the `.murder/` knowledge structure, dispatch to an agent backend, and track the conversation. Start with basic dispatch (#6), then layer on the validation loop (#7).
